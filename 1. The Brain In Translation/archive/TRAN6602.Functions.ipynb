{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8e24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Paragraphs extracted from XML files contain different numbers of sentences,\n",
    "    and even incomplete sentences. This function collapses a list of paragraphs\n",
    "    into a list of sentences or sentence-equivalents in advance of NLP processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_raw = \"\"\n",
    "    punc = set([\".\", \",\", \";\", \":\"])\n",
    "    for i, para in enumerate(text):\n",
    "        if para[-1] in punc:\n",
    "            doc_raw += (para + \" \")\n",
    "        else:\n",
    "            doc_raw += (para + \". \")\n",
    "        \n",
    "    return doc_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_gen(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is primarily for removing references, and fixing spacing\n",
    "    between/within sentences in preparation for scispaCy language modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    text = text.replace('\\u200a', '').replace('\\n', '')   # remove weird space code, newlines\n",
    "    text = re.sub('\\[(\\d+)\\]', '', text)                  # remove refs ([1], [23], etc.)\n",
    "    text = text.replace(' ,', '').replace(' .', '.')      # fix spaces created by prev line\n",
    "    text = text.replace(' ;', ';').replace(' :', ':')     # \n",
    "    text = text.replace('  ', ' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fdfb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_results_df():\n",
    "    \n",
    "    \"\"\"\n",
    "    Results DF should be reset for every hypothesis tested.\n",
    "    Note: Run BEFORE preprocess_tok to capture unanalyzed tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    results_df = corpus_new[['Group', 'Author', 'Title']]\n",
    "    results_df['Title'] = pd.Series([title[:31] for title in results_df['Title']])  # prune title for readability\n",
    "    \n",
    "    # with word count\n",
    "    for i in range(0,20):\n",
    "        # Pull text from df\n",
    "        text = corpus_new.loc[i, \"Text\"]\n",
    "        # Split by word (\" \" for simplicity)\n",
    "        word_ct = len(text.split(' '))\n",
    "        # Put in results_df\n",
    "        results_df.loc[i, 'Word Count'] = word_ct\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e3856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(ls):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a list of unique items from an existing list.\n",
    "    \"\"\"\n",
    " \n",
    "    unique_list = []\n",
    "     \n",
    "    for x in ls:\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f0b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tok(doc):\n",
    "    \n",
    "    # Collect lemmas not tagged by spaCy as 1. punctuation, 2. digits, 3. URLs, or 4. stop words\n",
    "    tokens = [tok.lemma_ for tok in doc if not (tok.is_punct | tok.is_digit | tok.like_url | tok.is_stop)]\n",
    "    \n",
    "    # Remove any tokens containing mid-string digits (e.g. \"P5-a\") or punc ('t(are')\n",
    "    tokens = [tok for tok in tokens if not re.search(\"\\d\", tok)]\n",
    "    tokens = [tok for tok in tokens if not re.search(\"\\(\", tok)]\n",
    "    tokens = [tok for tok in tokens if not re.search(\"\\)\", tok)]\n",
    "    \n",
    "    # (4.13) Break apart hyphen- or slash-separated compounds\n",
    "    seps = ['-', '–', '―',\n",
    "            ';', ':',\n",
    "            '\\]', '\\[', \n",
    "            '’', '”', \n",
    "            '>', '<', '/']\n",
    "    for sep in seps:\n",
    "        new_toks = []\n",
    "        for tok in tokens:\n",
    "            new_toks += tok.split(sep)\n",
    "        tokens = new_toks\n",
    "    \n",
    "    # (4.13) Remove remaining abbreviations\n",
    "    tokens = [tok for tok in tokens if not re.search(\"[a-zA-Z]\\.[a-zA-Z]\\.\", tok)]\n",
    "    tokens = [tok for tok in tokens if not re.search(\"\\+\", tok)]\n",
    "    \n",
    "    # Remove punc and small words (e.g. 'a', 'P', 'mm')\n",
    "    punc_to_skip = set(['±', '=', '>', '<'])\n",
    "    tokens = [tok for tok in tokens if tok not in punc_to_skip]     # can skip?\n",
    "    tokens = [tok for tok in tokens if len(tok) > 3]    \n",
    "       \n",
    "    # Unify to lowercase (to simplify matching)\n",
    "    tokens = [tok.lower() for tok in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tokens_unique_to_one_doc(i):\n",
    "    \n",
    "    other_docs = list(range(0,20))\n",
    "    other_docs.remove(i)\n",
    "    \n",
    "    # Doc in question\n",
    "    text_i = corpus_new.loc[i, \"Text\"]\n",
    "    doc_i = nlp(text_i)\n",
    "    tokens_i = unique(preprocess_tok(doc_i))\n",
    "    \n",
    "    # Iterate thru all 19 other docs\n",
    "    for j in other_docs:\n",
    "        text_j = corpus_new.loc[j, \"Text\"]\n",
    "        doc_j = nlp(text_j)\n",
    "        tokens_j = set(unique(preprocess_tok(doc_j)))\n",
    "        \n",
    "        for tok in tokens_i:\n",
    "            if tok in tokens_j:\n",
    "                tokens_i.remove(tok)\n",
    "    \n",
    "    return tokens_i            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6494c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexeme_counter(doc, string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function for getting raw lemma count in a document.\n",
    "    \"\"\"\n",
    "        \n",
    "    tokens = preprocess_tok(doc)\n",
    "    counter = 0\n",
    "    for tok in tokens:\n",
    "        if str(tok) == str(string):\n",
    "            counter += 1\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_token_ratio(doc):\n",
    "    \n",
    "    token_list = preprocess_tok(doc)\n",
    "    n_type = len(unique(token_list))\n",
    "    n_token = len(token_list)\n",
    "    ttr = n_type/n_token\n",
    "    \n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4497a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_means(var):\n",
    "    \n",
    "    from scipy.stats import ttest_ind\n",
    "    \n",
    "    jp_stats = list(results_df[results_df['Group'] == 'JP-EN'].loc[:, var])\n",
    "    en_stats = list(results_df[results_df['Group'] == 'EN-EN'].loc[:, var])\n",
    "    P = ttest_ind(jp_stats, en_stats).pvalue\n",
    "    \n",
    "    print(f'Mean {var}, JP-EN:  {np.mean(jp_stats)}')\n",
    "    print(f'Mean {var}, EN-EN:  {np.mean(en_stats)}')\n",
    "    print(f'Sig. (unpaired t-test): {P}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582371e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15373c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c4c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be52c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2734ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba911c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7371bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8562bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71c0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
