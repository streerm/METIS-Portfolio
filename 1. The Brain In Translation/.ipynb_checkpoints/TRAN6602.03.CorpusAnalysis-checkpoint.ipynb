{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4月12日) Corpus Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import re\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle\n",
    "corpus_new = pd.read_pickle('savefiles/corpusfull_20220410.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Paragraphs extracted from XML files contain different numbers of sentences,\n",
    "    and even incomplete sentences. This function collapses a list of paragraphs\n",
    "    into a list of sentences or sentence-equivalents in advance of NLP processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_raw = \"\"\n",
    "    punc = set([\".\", \",\", \";\", \":\"])\n",
    "    for i, para in enumerate(text):\n",
    "        if para[-1] in punc:\n",
    "            doc_raw += (para + \" \")\n",
    "        else:\n",
    "            doc_raw += (para + \". \")\n",
    "        \n",
    "    return doc_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_gen(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is primarily for removing references, and fixing spacing\n",
    "    between/within sentences in preparation for scispaCy language modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    import re\n",
    "\n",
    "    text = text.replace('\\u200a', '').replace('\\n', '')   # remove weird space code, newlines\n",
    "    text = re.sub('\\[(\\d+)\\]', '', text)                  # remove refs ([1], [23], etc.)\n",
    "    text = text.replace(' ,', '').replace(' .', '.')      # fix spaces created by prev line\n",
    "    text = text.replace(' ;', ';').replace(' :', ':')     # \n",
    "    text = text.replace('  ', ' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts each doc from list of paras to one long string\n",
    "for i in range(0,20):\n",
    "    corpus_new.loc[i, \"Text\"] = collapse(corpus_new.loc[i, \"Text\"])\n",
    "\n",
    "# Preprocess each doc before Spacy modeling\n",
    "for i in range(0,20):\n",
    "    corpus_new.loc[i, \"Text\"] = preprocess_gen(corpus_new.loc[i, \"Text\"])\n",
    "    \n",
    "# For practicing on single texts\n",
    "# text = corpus_new.loc[0, \"Text\"]\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_results_df():\n",
    "    \n",
    "    \"\"\"\n",
    "    Results DF should be reset for every hypothesis tested.\n",
    "    \"\"\"\n",
    "    \n",
    "    results_df = corpus_new[['Group', 'Author', 'Title']]\n",
    "    results_df['Title'] = pd.Series([title[:31] for title in results_df['Title']])  # prune title for readability\n",
    "    \n",
    "    # with word count\n",
    "    for i in range(0,20):\n",
    "        # Pull text from df\n",
    "        text = corpus_new.loc[i, \"Text\"]\n",
    "        # Split by word (\" \" for simplicity)\n",
    "        word_ct = len(text.split(' '))\n",
    "        # Put in results_df\n",
    "        results_df.loc[i, 'Word Count'] = word_ct\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(ls):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a list of unique items from an existing list.\n",
    "    \"\"\"\n",
    " \n",
    "    unique_list = []\n",
    "     \n",
    "    for x in ls:\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ttr(doc):\n",
    "    \n",
    "    # Collect lemmas not tagged by spaCy as 1. punctuation, 2. digits, 3. URLs, or 4. stop words\n",
    "    tokens = [tok.lemma_ for tok in doc if not (tok.is_punct | tok.is_digit | tok.like_url | tok.is_stop)]\n",
    "    \n",
    "    # Remove any tokens containing mid-string digits (e.g. \"P5-a\") or punc ('t(are')\n",
    "    tokens = [tok for tok in tokens if not re.search(\"\\d\", tok)]\n",
    "    tokens = [tok for tok in tokens if not re.search(\"\\(\", tok)]\n",
    "    tokens = [tok for tok in tokens if not re.search(\"\\)\", tok)]\n",
    "    \n",
    "    # Remove punc and small words (e.g. 'a', 'P', 'mm')\n",
    "    punc_to_skip = set(['±', '=', '>', '<'])\n",
    "    tokens = [tok for tok in tokens if tok not in punc_to_skip]     # can skip?\n",
    "    tokens = [tok for tok in tokens if len(tok) > 3]\n",
    "    \n",
    "    # Unify to lowercase (to simplify matching)\n",
    "    tokens = [tok.lower() for tok in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_token_ratio(doc):\n",
    "    \n",
    "    token_list = preprocess_ttr(doc)\n",
    "    n_type = len(unique(token_list))\n",
    "    n_token = len(token_list)\n",
    "    ttr = n_type/n_token\n",
    "    \n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tokens_unique_to_one_doc(i):\n",
    "    \n",
    "    other_docs = list(range(0,20))\n",
    "    other_docs.remove(i)\n",
    "    \n",
    "    # Doc in question\n",
    "    text_i = corpus_new.loc[i, \"Text\"]\n",
    "    doc_i = nlp(text_i)\n",
    "    tokens_i = unique(preprocess_ttr(doc_i))\n",
    "    \n",
    "    # Iterate thru all 19 other docs\n",
    "    for j in other_docs:\n",
    "        text_j = corpus_new.loc[j, \"Text\"]\n",
    "        doc_j = nlp(text_j)\n",
    "        tokens_j = set(unique(preprocess_ttr(doc_j)))\n",
    "        \n",
    "        for tok in tokens_i:\n",
    "            if tok in tokens_j:\n",
    "                tokens_i.remove(tok)\n",
    "    \n",
    "    return tokens_i            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['full-blown',\n",
       " 'humanization',\n",
       " 'prosocial',\n",
       " 'sacrifice',\n",
       " 'save',\n",
       " 'utilitarian',\n",
       " 'victim',\n",
       " 'humanness',\n",
       " 'vicarious',\n",
       " 'fictitious',\n",
       " 'humanize',\n",
       " 'humanized',\n",
       " 'pgacc/mofc',\n",
       " 'insula/ifg',\n",
       " 'precuneus/pcc',\n",
       " 'other-related',\n",
       " 'human-like',\n",
       " 'deeply',\n",
       " 'living',\n",
       " 'reciprocity',\n",
       " 'coalition',\n",
       " 'greatly',\n",
       " 'belief',\n",
       " 'conspecific',\n",
       " 'anthropomorphism',\n",
       " 'deny',\n",
       " 'dehumanization',\n",
       " 'outgroup',\n",
       " 'dehumanized',\n",
       " 'historic',\n",
       " 'barbarian',\n",
       " 'deserving',\n",
       " 'justify',\n",
       " 'cohesion',\n",
       " 'harmful',\n",
       " 'first-hand',\n",
       " 'intense',\n",
       " 'embody',\n",
       " 'reluctance',\n",
       " 'kill',\n",
       " 'non-utilitarian',\n",
       " 'refrain',\n",
       " 'death',\n",
       " 'respondent',\n",
       " 'trolley',\n",
       " 'rail',\n",
       " 'worker',\n",
       " 'confront',\n",
       " 'pull',\n",
       " 'modification',\n",
       " 'footbridge',\n",
       " 'heavy',\n",
       " 'rucksack',\n",
       " 'proximity',\n",
       " 'mechanic',\n",
       " 'greene',\n",
       " 'agent-authored',\n",
       " 'morally',\n",
       " 'conceive',\n",
       " 'shock',\n",
       " 'underpinning',\n",
       " 'morality',\n",
       " 'valuation',\n",
       " 'care-based',\n",
       " 'justice-based',\n",
       " 'abovementioned',\n",
       " 'fronto-insular',\n",
       " 'supposedly',\n",
       " 'deliberation',\n",
       " 'blend',\n",
       " 'allegedly',\n",
       " 'emotional-motivational',\n",
       " 'acceptability',\n",
       " 'warmth',\n",
       " 'addict',\n",
       " 'drawback',\n",
       " 'importunate',\n",
       " 'idiosyncratic',\n",
       " 'uniform',\n",
       " 'well-controlled',\n",
       " 'unchanged',\n",
       " 'non-humanized',\n",
       " 're-instantiate',\n",
       " 'prospective',\n",
       " 'coding',\n",
       " 'midcingulate',\n",
       " 'head-movement',\n",
       " 'anatomic',\n",
       " 'malformation',\n",
       " 'vienna',\n",
       " 'factual',\n",
       " 'photo',\n",
       " 'radboud',\n",
       " 'faces',\n",
       " 'shot',\n",
       " 'concerned',\n",
       " 'mechanical',\n",
       " 'elaborately',\n",
       " 'plos',\n",
       " 'publication',\n",
       " 'twenty-four',\n",
       " 'hauser',\n",
       " 'seriously',\n",
       " 'injure',\n",
       " 'alive',\n",
       " 'imaginary',\n",
       " 'mortal',\n",
       " 'homogeneously',\n",
       " 'printed',\n",
       " 'five-point',\n",
       " 'valuable',\n",
       " 'attractive',\n",
       " 'reverse-coded',\n",
       " 'opinion',\n",
       " 'permute',\n",
       " 'assignment',\n",
       " 'portrait',\n",
       " 'willingness',\n",
       " 'pseudorandomize',\n",
       " 'press-triggered',\n",
       " 'emergency',\n",
       " 'succeed',\n",
       " 'proceed',\n",
       " 'decisive',\n",
       " 'supine',\n",
       " 'thigh',\n",
       " 'francisco',\n",
       " 'primed',\n",
       " 'intertrial',\n",
       " 'post-experimental',\n",
       " 'pasw',\n",
       " 'chicago',\n",
       " 'bonferroni-correction',\n",
       " 'alpha',\n",
       " 'reception',\n",
       " 'generalized',\n",
       " 'autocalibrating',\n",
       " 'grappa',\n",
       " 'co-planar',\n",
       " 'sinc',\n",
       " 'algorithm',\n",
       " 'spinal',\n",
       " 'consortium',\n",
       " 'european',\n",
       " 'nonlinear',\n",
       " 'non-normalized',\n",
       " 'fmrib',\n",
       " 'non-brain',\n",
       " 'full-width-at-half-maximum',\n",
       " 'concatenation',\n",
       " 'square-wave',\n",
       " 'rigid-body',\n",
       " 'scan-by-scan',\n",
       " 'dilemmas',\n",
       " 'substantiate',\n",
       " 'eigenvariate',\n",
       " 'coordinate-based',\n",
       " 'dorsally',\n",
       " 'zone',\n",
       " 'newly',\n",
       " 'shorten',\n",
       " 'seat',\n",
       " 'soundproof',\n",
       " 'keyword',\n",
       " 'slider',\n",
       " 'self-oriented',\n",
       " 'tear',\n",
       " 'other-oriented',\n",
       " 'compassion',\n",
       " 'well-being',\n",
       " 'consistency',\n",
       " 'cronbach',\n",
       " 'tangible',\n",
       " 'interchangeable',\n",
       " 'reverse-code',\n",
       " 'permuted',\n",
       " 'aggregated',\n",
       " 'imminent',\n",
       " 'post-fmri',\n",
       " 'measuring',\n",
       " 'bonferroni-corrected',\n",
       " 'recode',\n",
       " 'greenhouse-geisser',\n",
       " 'aggregate',\n",
       " 'precuneus/posterior',\n",
       " 'pgacc',\n",
       " 'perigenual',\n",
       " 'mofc',\n",
       " 'convention',\n",
       " 'mid-orbital',\n",
       " 'rightward',\n",
       " 'sub-',\n",
       " 'mofg',\n",
       " 'insula/inferior',\n",
       " 'cingulated',\n",
       " 'specification',\n",
       " 'amcc/rcz',\n",
       " 'gyrus/tpj',\n",
       " 'frontoinsular',\n",
       " 'mtg/temporopolar',\n",
       " 'reflection',\n",
       " 'fmri-experiment',\n",
       " 'fairly',\n",
       " 'self-related',\n",
       " 'confrontation',\n",
       " 'self-',\n",
       " 'theorize',\n",
       " 'withdrawal',\n",
       " 'pmcc',\n",
       " 'midtemporal',\n",
       " 'appeal',\n",
       " 'obligation',\n",
       " 'pgacc/vmpfc/mofc',\n",
       " 'undisputed',\n",
       " 'interconnected',\n",
       " 'co-activate',\n",
       " 'warn',\n",
       " 'valuate',\n",
       " 'weigh',\n",
       " 'quick',\n",
       " 'rational',\n",
       " 'presumed',\n",
       " 'adjoin',\n",
       " 'known',\n",
       " 'speculative',\n",
       " 'perspective-taking',\n",
       " 'unaware',\n",
       " 'cell',\n",
       " 'imagination',\n",
       " 'distressing',\n",
       " 'partiality',\n",
       " 'shed',\n",
       " 'altruistic',\n",
       " 'oneness',\n",
       " 'alleviate',\n",
       " 'discern',\n",
       " 'counteract',\n",
       " 'clear-cut',\n",
       " 'riddle']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = find_tokens_unique_to_one_doc(3)\n",
    "toks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For counting 1-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexeme_counter(doc, string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function for getting raw lemma count in a document.\n",
    "    \"\"\"\n",
    "        \n",
    "    tokens = preprocess_ttr(doc)\n",
    "    counter = 0\n",
    "    for tok in tokens:\n",
    "        if str(tok) == str(string):\n",
    "            counter += 1\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>n_think</th>\n",
       "      <th>n_think_adj</th>\n",
       "      <th>n_consider</th>\n",
       "      <th>n_consider_adj</th>\n",
       "      <th>n_report</th>\n",
       "      <th>n_report_adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Tamura</td>\n",
       "      <td>Neural Network Development in L</td>\n",
       "      <td>6926.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.002888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Watanabe</td>\n",
       "      <td>Diminished Medial Prefrontal Ac</td>\n",
       "      <td>6918.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.001012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Sobhani</td>\n",
       "      <td>Interpersonal Liking Modulates</td>\n",
       "      <td>5953.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Majdandžić</td>\n",
       "      <td>The Human Factor: Behavioral an</td>\n",
       "      <td>9587.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Dixon</td>\n",
       "      <td>The Decision to Engage Cognitiv</td>\n",
       "      <td>8620.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Ohta</td>\n",
       "      <td>Syntactic Computation in the Hu</td>\n",
       "      <td>8739.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.001144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Deeley</td>\n",
       "      <td>Using Hypnotic Suggestion to Mo</td>\n",
       "      <td>7103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.001689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Pawliczek</td>\n",
       "      <td>Anger under Control: Neural Cor</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Jansma</td>\n",
       "      <td>fMRI Guided rTMS Evidence for R</td>\n",
       "      <td>4865.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Lidzba</td>\n",
       "      <td>Complex Visual Search in Childr</td>\n",
       "      <td>4478.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Minamoto</td>\n",
       "      <td>Extrapunitive and Intropunitive</td>\n",
       "      <td>6109.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Itahashi</td>\n",
       "      <td>Altered Network Topologies and</td>\n",
       "      <td>8162.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.001960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Yomogida</td>\n",
       "      <td>The Neural Basis of Event Simul</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Murakami</td>\n",
       "      <td>Neural Networks for Mindfulness</td>\n",
       "      <td>4944.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Higashiyama</td>\n",
       "      <td>The Neural Basis of Typewriting</td>\n",
       "      <td>7880.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.002792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Neale</td>\n",
       "      <td>Functional Activation during th</td>\n",
       "      <td>6679.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Kim</td>\n",
       "      <td>Identifying Core Affect in Indi</td>\n",
       "      <td>6938.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Nakai</td>\n",
       "      <td>Sense of Accomplishment Is Modu</td>\n",
       "      <td>7092.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.001128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Himmelstein</td>\n",
       "      <td>Linguistic analysis of the auto</td>\n",
       "      <td>5621.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Nakata</td>\n",
       "      <td>Negative BOLD responses during</td>\n",
       "      <td>4696.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.001917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Group       Author                            Title  Word Count  n_think  \\\n",
       "0   JP-EN       Tamura  Neural Network Development in L      6926.0      2.0   \n",
       "1   JP-EN     Watanabe  Diminished Medial Prefrontal Ac      6918.0      1.0   \n",
       "2   EN-EN      Sobhani  Interpersonal Liking Modulates       5953.0      1.0   \n",
       "3   EN-EN   Majdandžić  The Human Factor: Behavioral an      9587.0      4.0   \n",
       "4   EN-EN        Dixon  The Decision to Engage Cognitiv      8620.0      1.0   \n",
       "5   JP-EN         Ohta  Syntactic Computation in the Hu      8739.0      2.0   \n",
       "6   EN-EN       Deeley  Using Hypnotic Suggestion to Mo      7103.0      0.0   \n",
       "7   EN-EN    Pawliczek  Anger under Control: Neural Cor      4688.0      0.0   \n",
       "8   EN-EN       Jansma  fMRI Guided rTMS Evidence for R      4865.0      0.0   \n",
       "9   EN-EN       Lidzba  Complex Visual Search in Childr      4478.0      1.0   \n",
       "10  JP-EN     Minamoto  Extrapunitive and Intropunitive      6109.0      7.0   \n",
       "11  JP-EN     Itahashi  Altered Network Topologies and       8162.0      4.0   \n",
       "12  JP-EN     Yomogida  The Neural Basis of Event Simul      7057.0      6.0   \n",
       "13  JP-EN     Murakami  Neural Networks for Mindfulness      4944.0      0.0   \n",
       "14  JP-EN  Higashiyama  The Neural Basis of Typewriting      7880.0      2.0   \n",
       "15  EN-EN        Neale  Functional Activation during th      6679.0      0.0   \n",
       "16  EN-EN          Kim  Identifying Core Affect in Indi      6938.0      0.0   \n",
       "17  JP-EN        Nakai  Sense of Accomplishment Is Modu      7092.0      0.0   \n",
       "18  EN-EN  Himmelstein  Linguistic analysis of the auto      5621.0      2.0   \n",
       "19  JP-EN       Nakata  Negative BOLD responses during       4696.0      1.0   \n",
       "\n",
       "    n_think_adj  n_consider  n_consider_adj  n_report  n_report_adj  \n",
       "0      0.000289         4.0        0.000578      20.0      0.002888  \n",
       "1      0.000145         3.0        0.000434       7.0      0.001012  \n",
       "2      0.000168         3.0        0.000504       3.0      0.000504  \n",
       "3      0.000417         7.0        0.000730       1.0      0.000104  \n",
       "4      0.000116         3.0        0.000348       2.0      0.000232  \n",
       "5      0.000229         1.0        0.000114      10.0      0.001144  \n",
       "6      0.000000         0.0        0.000000      12.0      0.001689  \n",
       "7      0.000000         2.0        0.000427      15.0      0.003200  \n",
       "8      0.000000         1.0        0.000206       2.0      0.000411  \n",
       "9      0.000223         2.0        0.000447       0.0      0.000000  \n",
       "10     0.001146        10.0        0.001637       6.0      0.000982  \n",
       "11     0.000490         5.0        0.000613      16.0      0.001960  \n",
       "12     0.000850         1.0        0.000142       5.0      0.000709  \n",
       "13     0.000000         2.0        0.000405       4.0      0.000809  \n",
       "14     0.000254         4.0        0.000508      22.0      0.002792  \n",
       "15     0.000000        10.0        0.001497       0.0      0.000000  \n",
       "16     0.000000         1.0        0.000144       8.0      0.001153  \n",
       "17     0.000000         7.0        0.000987       8.0      0.001128  \n",
       "18     0.000356         1.0        0.000178       2.0      0.000356  \n",
       "19     0.000213         1.0        0.000213       9.0      0.001917  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = initialize_results_df()\n",
    "\n",
    "for lexeme in [\"think\", \"consider\", \"report\"]:\n",
    "    \n",
    "    new_name_n = \"n_\" + str(lexeme)\n",
    "    new_name_adj = str(new_name_n) + \"_adj\"\n",
    "    \n",
    "    for i in range(0,20):\n",
    "        # Pull text from df\n",
    "        text = corpus_new.loc[i, \"Text\"]\n",
    "        # Run scispaCy model\n",
    "        doc = nlp(text)\n",
    "        # Count lexeme (includes preprocessing)\n",
    "        ct = lexeme_counter(doc, lexeme)\n",
    "        # Put in results_df\n",
    "        results_df.loc[i, new_name_n] = ct\n",
    "        results_df.loc[i, new_name_adj] = (ct / results_df.loc[i, 'Word Count']) #* np.mean(results_df['Word Count'])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean n_think, JP-EN:  2.5\n",
      "Mean n_think, EN-EN:  0.9\n",
      "Sig. (unpaired t-test): 0.08096764680310031\n",
      "\n",
      "\n",
      "Mean n_consider, JP-EN:  3.8\n",
      "Mean n_consider, EN-EN:  3.0\n",
      "Sig. (unpaired t-test): 0.5626770820919398\n",
      "\n",
      "\n",
      "Mean n_think_adj, JP-EN:  0.00036150768325888323\n",
      "Mean n_think_adj, EN-EN:  0.00012803460328847217\n",
      "Sig. (unpaired t-test): 0.08375907208446082\n",
      "\n",
      "\n",
      "Mean n_consider_adj, JP-EN:  0.0005628961972975831\n",
      "Mean n_consider_adj, EN-EN:  0.0004480197983520875\n",
      "Sig. (unpaired t-test): 0.5674801108857972\n",
      "\n",
      "\n",
      "Mean n_report, JP-EN:  10.7\n",
      "Mean n_report, EN-EN:  4.5\n",
      "Sig. (unpaired t-test): 0.029472239581535983\n",
      "\n",
      "\n",
      "Mean n_report_adj, JP-EN:  0.0015340292107768054\n",
      "Mean n_report_adj, EN-EN:  0.0007649338089320477\n",
      "Sig. (unpaired t-test): 0.07608897487248438\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_means('n_think')\n",
    "compare_means('n_consider')\n",
    "compare_means('n_think_adj')\n",
    "compare_means('n_consider_adj')\n",
    "compare_means('n_report')\n",
    "compare_means('n_report_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    results_df.loc[i, 'Combined'] = results_df.loc[i, 'n_think'] + results_df.loc[i, 'n_consider']\n",
    "    results_df.loc[i, 'Combined_adj'] = results_df.loc[i, 'Combined'] / results_df.loc[i, 'Word Count']\n",
    "    \n",
    "compare_means('Combined')\n",
    "compare_means('Combined_adj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 2,3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>n_inthisstudy</th>\n",
       "      <th>n_inthisstudy_adj</th>\n",
       "      <th>n_therefore</th>\n",
       "      <th>n_therefore_adj</th>\n",
       "      <th>n_inaddition</th>\n",
       "      <th>n_inaddition_adj</th>\n",
       "      <th>n_boldresponse</th>\n",
       "      <th>n_boldresponse_adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Tamura</td>\n",
       "      <td>Neural Network Development in L</td>\n",
       "      <td>6926.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Watanabe</td>\n",
       "      <td>Diminished Medial Prefrontal Ac</td>\n",
       "      <td>6918.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Sobhani</td>\n",
       "      <td>Interpersonal Liking Modulates</td>\n",
       "      <td>5953.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Majdandžić</td>\n",
       "      <td>The Human Factor: Behavioral an</td>\n",
       "      <td>9587.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Dixon</td>\n",
       "      <td>The Decision to Engage Cognitiv</td>\n",
       "      <td>8620.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Ohta</td>\n",
       "      <td>Syntactic Computation in the Hu</td>\n",
       "      <td>8739.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Deeley</td>\n",
       "      <td>Using Hypnotic Suggestion to Mo</td>\n",
       "      <td>7103.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Pawliczek</td>\n",
       "      <td>Anger under Control: Neural Cor</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Jansma</td>\n",
       "      <td>fMRI Guided rTMS Evidence for R</td>\n",
       "      <td>4865.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Lidzba</td>\n",
       "      <td>Complex Visual Search in Childr</td>\n",
       "      <td>4478.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Minamoto</td>\n",
       "      <td>Extrapunitive and Intropunitive</td>\n",
       "      <td>6109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Itahashi</td>\n",
       "      <td>Altered Network Topologies and</td>\n",
       "      <td>8162.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Yomogida</td>\n",
       "      <td>The Neural Basis of Event Simul</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Murakami</td>\n",
       "      <td>Neural Networks for Mindfulness</td>\n",
       "      <td>4944.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Higashiyama</td>\n",
       "      <td>The Neural Basis of Typewriting</td>\n",
       "      <td>7880.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Neale</td>\n",
       "      <td>Functional Activation during th</td>\n",
       "      <td>6679.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Kim</td>\n",
       "      <td>Identifying Core Affect in Indi</td>\n",
       "      <td>6938.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Nakai</td>\n",
       "      <td>Sense of Accomplishment Is Modu</td>\n",
       "      <td>7092.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>EN-EN</td>\n",
       "      <td>Himmelstein</td>\n",
       "      <td>Linguistic analysis of the auto</td>\n",
       "      <td>5621.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>JP-EN</td>\n",
       "      <td>Nakata</td>\n",
       "      <td>Negative BOLD responses during</td>\n",
       "      <td>4696.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.008944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Group       Author                            Title  Word Count  \\\n",
       "0   JP-EN       Tamura  Neural Network Development in L      6926.0   \n",
       "1   JP-EN     Watanabe  Diminished Medial Prefrontal Ac      6918.0   \n",
       "2   EN-EN      Sobhani  Interpersonal Liking Modulates       5953.0   \n",
       "3   EN-EN   Majdandžić  The Human Factor: Behavioral an      9587.0   \n",
       "4   EN-EN        Dixon  The Decision to Engage Cognitiv      8620.0   \n",
       "5   JP-EN         Ohta  Syntactic Computation in the Hu      8739.0   \n",
       "6   EN-EN       Deeley  Using Hypnotic Suggestion to Mo      7103.0   \n",
       "7   EN-EN    Pawliczek  Anger under Control: Neural Cor      4688.0   \n",
       "8   EN-EN       Jansma  fMRI Guided rTMS Evidence for R      4865.0   \n",
       "9   EN-EN       Lidzba  Complex Visual Search in Childr      4478.0   \n",
       "10  JP-EN     Minamoto  Extrapunitive and Intropunitive      6109.0   \n",
       "11  JP-EN     Itahashi  Altered Network Topologies and       8162.0   \n",
       "12  JP-EN     Yomogida  The Neural Basis of Event Simul      7057.0   \n",
       "13  JP-EN     Murakami  Neural Networks for Mindfulness      4944.0   \n",
       "14  JP-EN  Higashiyama  The Neural Basis of Typewriting      7880.0   \n",
       "15  EN-EN        Neale  Functional Activation during th      6679.0   \n",
       "16  EN-EN          Kim  Identifying Core Affect in Indi      6938.0   \n",
       "17  JP-EN        Nakai  Sense of Accomplishment Is Modu      7092.0   \n",
       "18  EN-EN  Himmelstein  Linguistic analysis of the auto      5621.0   \n",
       "19  JP-EN       Nakata  Negative BOLD responses during       4696.0   \n",
       "\n",
       "    n_inthisstudy  n_inthisstudy_adj  n_therefore  n_therefore_adj  \\\n",
       "0             4.0           0.000578          1.0         0.000144   \n",
       "1             1.0           0.000145          7.0         0.001012   \n",
       "2             3.0           0.000504          1.0         0.000168   \n",
       "3             0.0           0.000000          3.0         0.000313   \n",
       "4             0.0           0.000000          3.0         0.000348   \n",
       "5             0.0           0.000000          5.0         0.000572   \n",
       "6             1.0           0.000141          8.0         0.001126   \n",
       "7             0.0           0.000000          6.0         0.001280   \n",
       "8             1.0           0.000206          0.0         0.000000   \n",
       "9             1.0           0.000223          2.0         0.000447   \n",
       "10            0.0           0.000000          8.0         0.001310   \n",
       "11           26.0           0.003185          5.0         0.000613   \n",
       "12            2.0           0.000283          0.0         0.000000   \n",
       "13            0.0           0.000000          4.0         0.000809   \n",
       "14            2.0           0.000254          8.0         0.001015   \n",
       "15            4.0           0.000599          3.0         0.000449   \n",
       "16            1.0           0.000144          1.0         0.000144   \n",
       "17            0.0           0.000000          3.0         0.000423   \n",
       "18            0.0           0.000000          1.0         0.000178   \n",
       "19            2.0           0.000426          2.0         0.000426   \n",
       "\n",
       "    n_inaddition  n_inaddition_adj  n_boldresponse  n_boldresponse_adj  \n",
       "0           11.0          0.001588             0.0            0.000000  \n",
       "1            3.0          0.000434             0.0            0.000000  \n",
       "2            4.0          0.000672             0.0            0.000000  \n",
       "3            5.0          0.000522             0.0            0.000000  \n",
       "4            1.0          0.000116             3.0            0.000348  \n",
       "5            2.0          0.000229             0.0            0.000000  \n",
       "6            3.0          0.000422             0.0            0.000000  \n",
       "7            1.0          0.000213             0.0            0.000000  \n",
       "8            1.0          0.000206             0.0            0.000000  \n",
       "9            4.0          0.000893             1.0            0.000223  \n",
       "10           3.0          0.000491             0.0            0.000000  \n",
       "11           9.0          0.001103             0.0            0.000000  \n",
       "12           3.0          0.000425             0.0            0.000000  \n",
       "13           1.0          0.000202             0.0            0.000000  \n",
       "14           7.0          0.000888             0.0            0.000000  \n",
       "15           3.0          0.000449             1.0            0.000150  \n",
       "16           3.0          0.000432             0.0            0.000000  \n",
       "17           0.0          0.000000             0.0            0.000000  \n",
       "18           6.0          0.001067             0.0            0.000000  \n",
       "19           5.0          0.001065            42.0            0.008944  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = initialize_results_df()\n",
    "cols = []\n",
    "\n",
    "for lexeme in [\"in this study\", \"therefore\", \"in addition\", \"bold response\"]:\n",
    "    \n",
    "    new_name_n = \"n_\" + str(lexeme.replace(\" \", \"\"))\n",
    "    new_name_adj = str(new_name_n) + \"_adj\"\n",
    "    cols.append(new_name_n)\n",
    "    cols.append(new_name_adj)\n",
    "\n",
    "    for i in range(0,20):\n",
    "        text = corpus_new.loc[i, \"Text\"].lower()\n",
    "        ct = text.count(lexeme)\n",
    "        # Put in results_df\n",
    "        results_df.loc[i, new_name_n] = ct\n",
    "        results_df.loc[i, new_name_adj] = (ct / results_df.loc[i, 'Word Count']) #* np.mean(results_df['Word Count'])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean n_inthisstudy, JP-EN:  3.7\n",
      "Mean n_inthisstudy, EN-EN:  1.1\n",
      "Sig. (unpaired t-test): 0.3213334124330426\n",
      "\n",
      "\n",
      "Mean n_inthisstudy_adj, JP-EN:  0.00048706861612406925\n",
      "Mean n_inthisstudy_adj, EN-EN:  0.00018166228041333954\n",
      "Sig. (unpaired t-test): 0.34355427166588526\n",
      "\n",
      "\n",
      "Mean n_therefore, JP-EN:  4.3\n",
      "Mean n_therefore, EN-EN:  2.8\n",
      "Sig. (unpaired t-test): 0.22402026964038416\n",
      "\n",
      "\n",
      "Mean n_therefore_adj, JP-EN:  0.0006323718848864725\n",
      "Mean n_therefore_adj, EN-EN:  0.00044529173124458626\n",
      "Sig. (unpaired t-test): 0.3293363734101986\n",
      "\n",
      "\n",
      "Mean n_inaddition, JP-EN:  4.4\n",
      "Mean n_inaddition, EN-EN:  3.1\n",
      "Sig. (unpaired t-test): 0.31323440605148106\n",
      "\n",
      "\n",
      "Mean n_inaddition_adj, JP-EN:  0.0006424914450415286\n",
      "Mean n_inaddition_adj, EN-EN:  0.0004992948110014182\n",
      "Sig. (unpaired t-test): 0.44839753078284084\n",
      "\n",
      "\n",
      "Mean n_boldresponse, JP-EN:  4.2\n",
      "Mean n_boldresponse, EN-EN:  0.5\n",
      "Sig. (unpaired t-test): 0.39119419168286584\n",
      "\n",
      "\n",
      "Mean n_boldresponse_adj, JP-EN:  0.0008943781942078365\n",
      "Mean n_boldresponse_adj, EN-EN:  7.210648341095022e-05\n",
      "Sig. (unpaired t-test): 0.37051384410735666\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in cols:\n",
    "    compare_means(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical testing\n",
    "#### 1. Type/token ratio (lexical diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_means(var):\n",
    "    \n",
    "    from scipy.stats import ttest_ind\n",
    "    \n",
    "    jp_stats = list(results_df[results_df['Group'] == 'JP-EN'].loc[:, var])\n",
    "    en_stats = list(results_df[results_df['Group'] == 'EN-EN'].loc[:, var])\n",
    "    P = ttest_ind(jp_stats, en_stats).pvalue\n",
    "    \n",
    "    print(f'Mean {var}, JP-EN:  {np.mean(jp_stats)}')\n",
    "    print(f'Mean {var}, EN-EN:  {np.mean(en_stats)}')\n",
    "    print(f'Sig. (unpaired t-test): {P}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate raw type/token ratio (lemmatized)\n",
    "\n",
    "for i in range(0,20):\n",
    "    # Pull text from df\n",
    "    text = corpus_new.loc[i, \"Text\"]\n",
    "    # Run scispaCy model\n",
    "    doc = nlp(text)\n",
    "    # Preprocess\n",
    "    ttr = type_token_ratio(doc)\n",
    "    # Put in results_df\n",
    "    results_df.loc[i, 'Type/Token'] = ttr\n",
    "\n",
    "results_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate adjusted type/token ratio (lemmatized) divided by word count\n",
    "\n",
    "for i in range(0,20):\n",
    "    # Pull text from df\n",
    "    text = corpus_new.loc[i, \"Text\"]\n",
    "    # Split by word (\" \" for simplicity)\n",
    "    word_ct = len(text.split(' '))\n",
    "    # Put in results_df\n",
    "    results_df.loc[i, 'Word Count'] = word_ct\n",
    "    \n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide by mean word count of all documents\n",
    "results_df['TTR_adj'] = (results_df['Type/Token'] / results_df['Word Count']) * np.mean(results_df['Word Count'])\n",
    "    \n",
    "results_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mean stats between JP-EN and EN-EN corpora\n",
    "\n",
    "compare_means('Type/Token')\n",
    "compare_means('Word Count')\n",
    "compare_means('TTR_adj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments (4/10): H1 seems to be rejected. fMRI studies authored by Japanese scientists are just as lexically sophisticated as comparable docs authored by Anglophone counterparts. Perhaps this is a good thing: i.e., any differences discovered later are a product of linguistic features, rather than scientific knowledge/ignorance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the entities extracted by the mention detector. Note that they don't have types\n",
    "# like in SpaCy, and they are more general (e.g including verbs) - these are any spans\n",
    "# which might be an entity in UMLS, a large biomedical database.\n",
    "print(a.ents)\n",
    "\n",
    "\n",
    "#>>> (Myeloid derived suppressor cells,\n",
    "#     MDSC,\n",
    "#     immature,\n",
    "#     myeloid cells,\n",
    "#     immunosuppressive activity,\n",
    "#     accumulate,\n",
    "#     tumor-bearing mice,\n",
    "#     humans,\n",
    "#     cancer,\n",
    "#     hepatocellular carcinoma,\n",
    "#     HCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also visualise dependency parses\n",
    "# (This renders automatically inside a jupyter notebook!):\n",
    "from spacy import displacy\n",
    "# displacy.render(next(doc2.sents), style='dep', jupyter=True)\n",
    "displacy.render(a, style='dep', jupyter=True)\n",
    "\n",
    "# See below for the generated SVG.\n",
    "# Zoom your browser in a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
