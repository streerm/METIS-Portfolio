{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d386e32e",
   "metadata": {},
   "source": [
    "#### (4月12日) Corpus Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99017208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import re\n",
    "from tqdm import tqdm \n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad051ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle\n",
    "corpus_new = pd.read_pickle('savefiles/corpusfull_20220410.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8e24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Paragraphs extracted from XML files contain different numbers of sentences,\n",
    "    and even incomplete sentences. This function collapses a list of paragraphs\n",
    "    into a list of sentences or sentence-equivalents in advance of NLP processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_raw = \"\"\n",
    "    punc = set([\".\", \",\", \";\", \":\"])\n",
    "    for i, para in enumerate(text):\n",
    "        if para[-1] in punc:\n",
    "            doc_raw += (para + \" \")\n",
    "        else:\n",
    "            doc_raw += (para + \". \")\n",
    "        \n",
    "    return doc_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_gen(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is primarily for removing references, and fixing spacing\n",
    "    between/within sentences in preparation for scispaCy language modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace('\\u200a', '').replace('\\n', '')   # remove weird space code, newlines\n",
    "    text = re.sub('\\[(\\d+)\\]', '', text)                  # remove refs ([1], [23], etc.)\n",
    "    text = text.replace(' ,', '').replace(' .', '.')      # fix spaces created by prev line\n",
    "    text = text.replace(' ;', ';').replace(' :', ':')     # \n",
    "    text = text.replace('  ', ' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts each doc from list of paras to one long string\n",
    "for i in range(0,20):\n",
    "    corpus_new.loc[i, \"Text\"] = collapse(corpus_new.loc[i, \"Text\"])\n",
    "\n",
    "# Preprocess each doc before Spacy modeling\n",
    "for i in range(0,20):\n",
    "    corpus_new.loc[i, \"Text\"] = preprocess_gen(corpus_new.loc[i, \"Text\"])\n",
    "    \n",
    "# For practicing on single texts\n",
    "# text = corpus_new.loc[0, \"Text\"]\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeefcc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row indices of docs in respective corpora\n",
    "jpen_i = [0,  1,  5, 10, 11, 12, 13, 14, 17, 19]\n",
    "enen_i = [2,  3,  4,  6,  7,  8,  9, 15, 16, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fdfb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_results_df():\n",
    "    \n",
    "    \"\"\"\n",
    "    Results DF should be reset for every hypothesis tested.\n",
    "    Note: Run BEFORE preprocess_tok to capture unanalyzed tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    results_df = corpus_new[['Group', 'Author', 'Title']]\n",
    "    results_df['Title'] = pd.Series([title[:31] for title in results_df['Title']])  # prune title for readability\n",
    "    \n",
    "    # with word count\n",
    "    for i in range(0,20):\n",
    "        # Pull text from df\n",
    "        text = corpus_new.loc[i, \"Text\"]\n",
    "        # Split by word (\" \" for simplicity)\n",
    "        word_ct = len(text.split(' '))\n",
    "        # Put in results_df\n",
    "        results_df.loc[i, 'Word Count'] = word_ct\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92036ceb",
   "metadata": {},
   "source": [
    "#### Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fabb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e3856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(ls):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a list of unique items from an existing list.\n",
    "    \"\"\"\n",
    " \n",
    "    unique_list = []\n",
    "     \n",
    "    for x in ls:\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f0b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tok(doc):\n",
    "    \n",
    "    # Collect lemmas not tagged by spaCy as 1. punctuation, 2. digits, 3. URLs, or 4. stop words\n",
    "    tokens = [tok.lemma_ for tok in doc if not (tok.is_punct | tok.is_digit | tok.like_url | tok.is_stop)]\n",
    "    \n",
    "    # Remove any tokens containing mid-string digits (e.g. \"P5-a\") or punc ('t(are')\n",
    "    tokens = [tok for tok in tokens if not re.search(\"\\d\", tok)]\n",
    "    tokens = [tok for tok in tokens if not re.search(\"\\(\", tok)]\n",
    "    tokens = [tok for tok in tokens if not re.search(\"\\)\", tok)]\n",
    "    \n",
    "    # (4.13) Break apart hyphen- or slash-separated compounds\n",
    "    seps = ['-', '–', '―',\n",
    "            ';', ':',\n",
    "            '\\]', '\\[', \n",
    "            '’', '”', \n",
    "            '>', '<', '/']\n",
    "    for sep in seps:\n",
    "        new_toks = []\n",
    "        for tok in tokens:\n",
    "            new_toks += tok.split(sep)\n",
    "        tokens = new_toks\n",
    "    \n",
    "    # (4.13) Remove remaining abbreviations\n",
    "    tokens = [tok for tok in tokens if not re.search(\"[a-zA-Z]\\.[a-zA-Z]\\.\", tok)]\n",
    "    tokens = [tok for tok in tokens if not re.search(\"\\+\", tok)]\n",
    "    \n",
    "    # Remove punc and small words (e.g. 'a', 'P', 'mm')\n",
    "    punc_to_skip = set(['±', '=', '>', '<'])\n",
    "    tokens = [tok for tok in tokens if tok not in punc_to_skip]     # can skip?\n",
    "    tokens = [tok for tok in tokens if len(tok) > 3]    \n",
    "       \n",
    "    # Unify to lowercase (to simplify matching)\n",
    "    tokens = [tok.lower() for tok in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tokens_unique_to_one_doc(i):\n",
    "    \n",
    "    other_docs = list(range(0,20))\n",
    "    other_docs.remove(i)\n",
    "    \n",
    "    # Doc in question\n",
    "    text_i = corpus_new.loc[i, \"Text\"]\n",
    "    doc_i = nlp(text_i)\n",
    "    tokens_i = unique(preprocess_tok(doc_i))\n",
    "    \n",
    "    # Iterate thru all 19 other docs\n",
    "    for j in other_docs:\n",
    "        text_j = corpus_new.loc[j, \"Text\"]\n",
    "        doc_j = nlp(text_j)\n",
    "        tokens_j = set(unique(preprocess_tok(doc_j)))\n",
    "        \n",
    "        for tok in tokens_i:\n",
    "            if tok in tokens_j:\n",
    "                tokens_i.remove(tok)\n",
    "    \n",
    "    return tokens_i            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e67ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance of preprocessing function\n",
    "toks_unique = find_tokens_unique_to_one_doc(5)\n",
    "toks_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6c902",
   "metadata": {},
   "source": [
    "### (4.13) Comparing tokens not specific to one doc or corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tokens found in 2+ docs in JP-EN\n",
    "\n",
    "tokens_shared_jpen = []\n",
    "\n",
    "for i in tqdm(jpen_i):\n",
    "    text_i = corpus_new.loc[i, \"Text\"]\n",
    "    doc_i = nlp(text_i)\n",
    "    tokens_i = preprocess_tok(doc_i)\n",
    "    tokens_i_unique = find_tokens_unique_to_one_doc(i)\n",
    "    tokens_shared_i = [tok for tok in tokens_i if tok not in set(tokens_i_unique)]\n",
    "    for tok in tokens_shared_i:\n",
    "        tokens_shared_jpen.append(tok)\n",
    "\n",
    "tokens_shared_jpen_unique = unique(tokens_shared_jpen) # remove duplicates\n",
    "\n",
    "print(f'Non-document-specific tokens in JP-EN: {len(tokens_shared_jpen_unique)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2945e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tokens found in 2+ docs in EN-EN\n",
    "\n",
    "tokens_shared_enen = []\n",
    "\n",
    "for i in tqdm(enen_i):\n",
    "    text_i = corpus_new.loc[i, \"Text\"]\n",
    "    doc_i = nlp(text_i)\n",
    "    tokens_i = preprocess_tok(doc_i)\n",
    "    tokens_i_unique = find_tokens_unique_to_one_doc(i)\n",
    "    tokens_shared_i = [tok for tok in tokens_i if tok not in set(tokens_i_unique)]\n",
    "    for tok in tokens_shared_i:\n",
    "        tokens_shared_enen.append(tok)\n",
    "\n",
    "tokens_shared_enen_unique = unique(tokens_shared_enen)\n",
    "print(f'Non-document-specific tokens in EN-EN: {len(tokens_shared_enen_unique)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0161ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find intersection of tokens appearing in ≧4 docs (≧2 in each corpus) \n",
    "\n",
    "toks_to_analyze = [tok for tok in tokens_shared_jpen_unique if tok in set(tokens_shared_enen_unique)]\n",
    "print(f'{len(toks_to_analyze)} tokens to analyze')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54439a4",
   "metadata": {},
   "source": [
    "### (4.13) Run overnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0422ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big Kahuna\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "docs_df = initialize_results_df()\n",
    "toks_df = pd.DataFrame()\n",
    "\n",
    "for tok in tqdm(toks_to_analyze):\n",
    "    for i in range(0,20):\n",
    "        text = corpus_new.loc[i, \"Text\"]\n",
    "        doc = nlp(text)\n",
    "        toks_in_doc = preprocess_tok(doc)\n",
    "        counter = 0\n",
    "        for doctok in toks_in_doc:\n",
    "            if str(doctok) == str(tok):\n",
    "                counter += 1\n",
    "        docs_df.loc[i, \"count\"] = counter\n",
    "        docs_df.loc[i, \"count_adj\"] = (counter / docs_df.loc[i, 'Word Count']) * 1000  # per 1000 words\n",
    "    \n",
    "    # Separate values by group (JP-EN v. EN-EN)\n",
    "    jp_ct    = list(docs_df[docs_df['Group'] == 'JP-EN'].loc[:, \"count\"])\n",
    "    jp_ctadj = list(docs_df[docs_df['Group'] == 'JP-EN'].loc[:, \"count_adj\"])\n",
    "    en_ct    = list(docs_df[docs_df['Group'] == 'EN-EN'].loc[:, \"count\"])\n",
    "    en_ctadj = list(docs_df[docs_df['Group'] == 'EN-EN'].loc[:, \"count_adj\"])\n",
    "    \n",
    "    # Calculate means and run t-tests\n",
    "    jp_ct_mean    = np.mean(jp_ct)\n",
    "    jp_ctadj_mean = np.mean(jp_ctadj)\n",
    "    en_ct_mean    = np.mean(en_ct)\n",
    "    en_ctadj_mean = np.mean(en_ctadj)\n",
    "        \n",
    "    P_ct    = ttest_ind(jp_ct, en_ct).pvalue\n",
    "    P_ctadj = ttest_ind(jp_ctadj, en_ctadj).pvalue\n",
    "    \n",
    "    # Add to DF\n",
    "    entry = {'Token': tok,\n",
    "             'JP-EN Mean Count': jp_ct_mean, 'JP-EN Mean Freq': jp_ctadj_mean,\n",
    "             'EN-EN Mean Count': en_ct_mean, 'EN-EN Mean Freq': en_ctadj_mean,\n",
    "             'P (count)': P_ct, 'P (freq)': P_ctadj}\n",
    "    toks_df = toks_df.append(entry, ignore_index = True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_df.to_pickle('savefiles/toksdf_20220413.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d1b4d",
   "metadata": {},
   "source": [
    "### Word-level analysis (4/18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "233413ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_df = pd.read_pickle('savefiles/toksdf_20220413.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9219fc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>JP-EN Mean Count</th>\n",
       "      <th>JP-EN Mean Freq</th>\n",
       "      <th>EN-EN Mean Count</th>\n",
       "      <th>EN-EN Mean Freq</th>\n",
       "      <th>P (count)</th>\n",
       "      <th>P (freq)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>detailed</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.170757</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.038129</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.005128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>obtain</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.599597</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.195152</td>\n",
       "      <td>0.022020</td>\n",
       "      <td>0.008790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>common</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.586217</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.072855</td>\n",
       "      <td>0.015005</td>\n",
       "      <td>0.011672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>list</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.162510</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.012019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>thickness</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.184369</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.035410</td>\n",
       "      <td>0.010987</td>\n",
       "      <td>0.013301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>parametric</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.361579</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.036445</td>\n",
       "      <td>0.023064</td>\n",
       "      <td>0.014723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>respectively</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.832330</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.132969</td>\n",
       "      <td>0.035646</td>\n",
       "      <td>0.019794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>adopt</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.152203</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.025988</td>\n",
       "      <td>0.020350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>activate</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.107028</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.226077</td>\n",
       "      <td>0.034104</td>\n",
       "      <td>0.021462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>study</td>\n",
       "      <td>40.9</td>\n",
       "      <td>6.115922</td>\n",
       "      <td>24.6</td>\n",
       "      <td>4.143915</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.024996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>enhance</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.388717</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.088260</td>\n",
       "      <td>0.032196</td>\n",
       "      <td>0.026860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>sphere</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.179215</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.052909</td>\n",
       "      <td>0.057584</td>\n",
       "      <td>0.031699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>medial</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.699221</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.277912</td>\n",
       "      <td>0.035680</td>\n",
       "      <td>0.034557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>brain</td>\n",
       "      <td>30.5</td>\n",
       "      <td>4.506381</td>\n",
       "      <td>16.5</td>\n",
       "      <td>2.616636</td>\n",
       "      <td>0.041613</td>\n",
       "      <td>0.045085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>candidate</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.124348</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.014079</td>\n",
       "      <td>0.040347</td>\n",
       "      <td>0.045421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>planar</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.151440</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.090547</td>\n",
       "      <td>0.024770</td>\n",
       "      <td>0.046820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>table</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1.374789</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.718090</td>\n",
       "      <td>0.050660</td>\n",
       "      <td>0.048762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>evaluate</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.350136</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.068084</td>\n",
       "      <td>0.060354</td>\n",
       "      <td>0.049363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>brodmann</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.322370</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>0.078899</td>\n",
       "      <td>0.049850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>knowledge</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.315409</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.132439</td>\n",
       "      <td>0.038475</td>\n",
       "      <td>0.050354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>contribute</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.325710</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.088760</td>\n",
       "      <td>0.039952</td>\n",
       "      <td>0.051162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>recently</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.158960</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.032932</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.053821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>edinburgh</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.114844</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.042886</td>\n",
       "      <td>0.005163</td>\n",
       "      <td>0.057413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>input</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.283566</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.049002</td>\n",
       "      <td>0.083742</td>\n",
       "      <td>0.058343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>essential</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.097452</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.073940</td>\n",
       "      <td>0.062194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>uncorrected</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.534201</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.162468</td>\n",
       "      <td>0.042708</td>\n",
       "      <td>0.070662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>report</td>\n",
       "      <td>10.9</td>\n",
       "      <td>1.564837</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.788136</td>\n",
       "      <td>0.029980</td>\n",
       "      <td>0.073906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>confirm</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.444021</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.238901</td>\n",
       "      <td>0.189204</td>\n",
       "      <td>0.075033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>distance</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.124171</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.014079</td>\n",
       "      <td>0.073940</td>\n",
       "      <td>0.075716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>clarify</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.299237</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.089556</td>\n",
       "      <td>0.076783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>twenty</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.131927</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.076638</td>\n",
       "      <td>0.080357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>indicate</td>\n",
       "      <td>12.2</td>\n",
       "      <td>1.733549</td>\n",
       "      <td>6.8</td>\n",
       "      <td>1.044632</td>\n",
       "      <td>0.069274</td>\n",
       "      <td>0.082398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>dominant</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.101009</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.098220</td>\n",
       "      <td>0.083180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>corresponding</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.184048</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.064966</td>\n",
       "      <td>0.062589</td>\n",
       "      <td>0.083210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>system</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.931063</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.483789</td>\n",
       "      <td>0.087442</td>\n",
       "      <td>0.083615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>think</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.361508</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.128035</td>\n",
       "      <td>0.080968</td>\n",
       "      <td>0.083759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>different</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1.292669</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.752631</td>\n",
       "      <td>0.054449</td>\n",
       "      <td>0.085997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>gyrus</td>\n",
       "      <td>15.9</td>\n",
       "      <td>2.490560</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1.070683</td>\n",
       "      <td>0.108027</td>\n",
       "      <td>0.087971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>weak</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.056368</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.134596</td>\n",
       "      <td>0.088817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>statistical</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.692635</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.403551</td>\n",
       "      <td>0.077117</td>\n",
       "      <td>0.091617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Token  JP-EN Mean Count  JP-EN Mean Freq  EN-EN Mean Count  \\\n",
       "672        detailed               1.2         0.170757               0.2   \n",
       "888          obtain               4.3         0.599597               1.4   \n",
       "286          common               4.0         0.586217               0.5   \n",
       "728            list               1.2         0.162510               0.1   \n",
       "426       thickness               1.2         0.184369               0.2   \n",
       "524      parametric               2.5         0.361579               0.3   \n",
       "631    respectively               6.1         0.832330               0.9   \n",
       "798           adopt               1.2         0.152203               0.1   \n",
       "720        activate               7.6         1.107028               1.5   \n",
       "153           study              40.9         6.115922              24.6   \n",
       "238         enhance               2.6         0.388717               0.6   \n",
       "1002         sphere               1.2         0.179215               0.4   \n",
       "649          medial               4.4         0.699221               1.8   \n",
       "86            brain              30.5         4.506381              16.5   \n",
       "1220      candidate               0.9         0.124348               0.1   \n",
       "429          planar               1.0         0.151440               0.6   \n",
       "470           table               9.4         1.374789               4.6   \n",
       "837        evaluate               2.4         0.350136               0.5   \n",
       "577        brodmann               2.0         0.322370               0.4   \n",
       "829       knowledge               2.1         0.315409               0.8   \n",
       "660      contribute               2.1         0.325710               0.6   \n",
       "274        recently               1.1         0.158960               0.2   \n",
       "389       edinburgh               0.8         0.114844               0.2   \n",
       "699           input               2.1         0.283566               0.3   \n",
       "594       essential               0.7         0.097452               0.1   \n",
       "582     uncorrected               3.3         0.534201               0.9   \n",
       "160          report              10.9         1.564837               4.7   \n",
       "891         confirm               3.0         0.444021               1.8   \n",
       "620        distance               0.7         0.124171               0.1   \n",
       "977         clarify               2.0         0.299237               0.1   \n",
       "182          twenty               0.8         0.131927               0.1   \n",
       "83         indicate              12.2         1.733549               6.8   \n",
       "271        dominant               0.8         0.101009               0.1   \n",
       "541   corresponding               1.3         0.184048               0.4   \n",
       "370          system               6.2         0.931063               3.3   \n",
       "714           think               2.5         0.361508               0.9   \n",
       "125       different               8.5         1.292669               5.0   \n",
       "75            gyrus              15.9         2.490560               7.4   \n",
       "1041           weak               0.4         0.056368               0.1   \n",
       "523     statistical               4.6         0.692635               2.5   \n",
       "\n",
       "      EN-EN Mean Freq  P (count)  P (freq)  \n",
       "672          0.038129   0.002363  0.005128  \n",
       "888          0.195152   0.022020  0.008790  \n",
       "286          0.072855   0.015005  0.011672  \n",
       "728          0.017790   0.013442  0.012019  \n",
       "426          0.035410   0.010987  0.013301  \n",
       "524          0.036445   0.023064  0.014723  \n",
       "631          0.132969   0.035646  0.019794  \n",
       "798          0.010431   0.025988  0.020350  \n",
       "720          0.226077   0.034104  0.021462  \n",
       "153          4.143915   0.003081  0.024996  \n",
       "238          0.088260   0.032196  0.026860  \n",
       "1002         0.052909   0.057584  0.031699  \n",
       "649          0.277912   0.035680  0.034557  \n",
       "86           2.616636   0.041613  0.045085  \n",
       "1220         0.014079   0.040347  0.045421  \n",
       "429          0.090547   0.024770  0.046820  \n",
       "470          0.718090   0.050660  0.048762  \n",
       "837          0.068084   0.060354  0.049363  \n",
       "577          0.046404   0.078899  0.049850  \n",
       "829          0.132439   0.038475  0.050354  \n",
       "660          0.088760   0.039952  0.051162  \n",
       "274          0.032932   0.049900  0.053821  \n",
       "389          0.042886   0.005163  0.057413  \n",
       "699          0.049002   0.083742  0.058343  \n",
       "594          0.010431   0.073940  0.062194  \n",
       "582          0.162468   0.042708  0.070662  \n",
       "160          0.788136   0.029980  0.073906  \n",
       "891          0.238901   0.189204  0.075033  \n",
       "620          0.014079   0.073940  0.075716  \n",
       "977          0.010431   0.089556  0.076783  \n",
       "182          0.010431   0.076638  0.080357  \n",
       "83           1.044632   0.069274  0.082398  \n",
       "271          0.011601   0.098220  0.083180  \n",
       "541          0.064966   0.062589  0.083210  \n",
       "370          0.483789   0.087442  0.083615  \n",
       "714          0.128035   0.080968  0.083759  \n",
       "125          0.752631   0.054449  0.085997  \n",
       "75           1.070683   0.108027  0.087971  \n",
       "1041         0.010431   0.134596  0.088817  \n",
       "523          0.403551   0.077117  0.091617  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lemmas more common in JP-EN than EN-EN\n",
    "toks_jp = toks_df[toks_df[\"JP-EN Mean Freq\"] > toks_df[\"EN-EN Mean Freq\"]]\n",
    "toks_jp.sort_values(\"P (freq)\").head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c2205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd5905ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>JP-EN Mean Count</th>\n",
       "      <th>JP-EN Mean Freq</th>\n",
       "      <th>EN-EN Mean Count</th>\n",
       "      <th>EN-EN Mean Freq</th>\n",
       "      <th>P (count)</th>\n",
       "      <th>P (freq)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>detailed</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.170757</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.038129</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.005128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>obtain</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.599597</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.195152</td>\n",
       "      <td>0.022020</td>\n",
       "      <td>0.008790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>explore</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.028621</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.232113</td>\n",
       "      <td>0.007005</td>\n",
       "      <td>0.011141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>common</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.586217</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.072855</td>\n",
       "      <td>0.015005</td>\n",
       "      <td>0.011672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>list</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.162510</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.012019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>thickness</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.184369</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.035410</td>\n",
       "      <td>0.010987</td>\n",
       "      <td>0.013301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>parametric</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.361579</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.036445</td>\n",
       "      <td>0.023064</td>\n",
       "      <td>0.014723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>respectively</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.832330</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.132969</td>\n",
       "      <td>0.035646</td>\n",
       "      <td>0.019794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>adopt</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.152203</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.025988</td>\n",
       "      <td>0.020350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>activate</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.107028</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.226077</td>\n",
       "      <td>0.034104</td>\n",
       "      <td>0.021462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>study</td>\n",
       "      <td>40.9</td>\n",
       "      <td>6.115922</td>\n",
       "      <td>24.6</td>\n",
       "      <td>4.143915</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.024996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>alternative</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.014438</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.131495</td>\n",
       "      <td>0.017920</td>\n",
       "      <td>0.025221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>enhance</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.388717</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.088260</td>\n",
       "      <td>0.032196</td>\n",
       "      <td>0.026860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>active</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.079231</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.425237</td>\n",
       "      <td>0.039044</td>\n",
       "      <td>0.028967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>remove</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.073286</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.205185</td>\n",
       "      <td>0.046477</td>\n",
       "      <td>0.031685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>sphere</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.179215</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.052909</td>\n",
       "      <td>0.057584</td>\n",
       "      <td>0.031699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>medial</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.699221</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.277912</td>\n",
       "      <td>0.035680</td>\n",
       "      <td>0.034557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>detection</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.011443</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.124554</td>\n",
       "      <td>0.045270</td>\n",
       "      <td>0.039317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>fill</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.011443</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.088988</td>\n",
       "      <td>0.054115</td>\n",
       "      <td>0.042676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>projection</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.011443</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.098515</td>\n",
       "      <td>0.054115</td>\n",
       "      <td>0.043092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Token  JP-EN Mean Count  JP-EN Mean Freq  EN-EN Mean Count  \\\n",
       "672       detailed               1.2         0.170757               0.2   \n",
       "888         obtain               4.3         0.599597               1.4   \n",
       "1284       explore               0.2         0.028621               1.4   \n",
       "286         common               4.0         0.586217               0.5   \n",
       "728           list               1.2         0.162510               0.1   \n",
       "426      thickness               1.2         0.184369               0.2   \n",
       "524     parametric               2.5         0.361579               0.3   \n",
       "631   respectively               6.1         0.832330               0.9   \n",
       "798          adopt               1.2         0.152203               0.1   \n",
       "720       activate               7.6         1.107028               1.5   \n",
       "153          study              40.9         6.115922              24.6   \n",
       "804    alternative               0.1         0.014438               0.8   \n",
       "238        enhance               2.6         0.388717               0.6   \n",
       "723         active               0.5         0.079231               2.8   \n",
       "984         remove               0.5         0.073286               1.3   \n",
       "1002        sphere               1.2         0.179215               0.4   \n",
       "649         medial               4.4         0.699221               1.8   \n",
       "1272     detection               0.1         0.011443               0.7   \n",
       "1208          fill               0.1         0.011443               0.6   \n",
       "1216    projection               0.1         0.011443               0.6   \n",
       "\n",
       "      EN-EN Mean Freq  P (count)  P (freq)  \n",
       "672          0.038129   0.002363  0.005128  \n",
       "888          0.195152   0.022020  0.008790  \n",
       "1284         0.232113   0.007005  0.011141  \n",
       "286          0.072855   0.015005  0.011672  \n",
       "728          0.017790   0.013442  0.012019  \n",
       "426          0.035410   0.010987  0.013301  \n",
       "524          0.036445   0.023064  0.014723  \n",
       "631          0.132969   0.035646  0.019794  \n",
       "798          0.010431   0.025988  0.020350  \n",
       "720          0.226077   0.034104  0.021462  \n",
       "153          4.143915   0.003081  0.024996  \n",
       "804          0.131495   0.017920  0.025221  \n",
       "238          0.088260   0.032196  0.026860  \n",
       "723          0.425237   0.039044  0.028967  \n",
       "984          0.205185   0.046477  0.031685  \n",
       "1002         0.052909   0.057584  0.031699  \n",
       "649          0.277912   0.035680  0.034557  \n",
       "1272         0.124554   0.045270  0.039317  \n",
       "1208         0.088988   0.054115  0.042676  \n",
       "1216         0.098515   0.054115  0.043092  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Still need to sort_values by P-value\n",
    "toks_df.sort_values(\"P (freq)\").head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f1a3018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>JP-EN Mean Count</th>\n",
       "      <th>JP-EN Mean Freq</th>\n",
       "      <th>EN-EN Mean Count</th>\n",
       "      <th>EN-EN Mean Freq</th>\n",
       "      <th>P (count)</th>\n",
       "      <th>P (freq)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>explore</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.028621</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.232113</td>\n",
       "      <td>0.007005</td>\n",
       "      <td>0.011141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>alternative</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.014438</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.131495</td>\n",
       "      <td>0.017920</td>\n",
       "      <td>0.025221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>active</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.079231</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.425237</td>\n",
       "      <td>0.039044</td>\n",
       "      <td>0.028967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>remove</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.073286</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.205185</td>\n",
       "      <td>0.046477</td>\n",
       "      <td>0.031685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>detection</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.011443</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.124554</td>\n",
       "      <td>0.045270</td>\n",
       "      <td>0.039317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>fill</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.011443</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.088988</td>\n",
       "      <td>0.054115</td>\n",
       "      <td>0.042676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>projection</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.011443</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.098515</td>\n",
       "      <td>0.054115</td>\n",
       "      <td>0.043092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>trio</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.077946</td>\n",
       "      <td>0.054371</td>\n",
       "      <td>0.043799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>vary</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.065803</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.265272</td>\n",
       "      <td>0.061052</td>\n",
       "      <td>0.045281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>establish</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.037324</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.187568</td>\n",
       "      <td>0.066324</td>\n",
       "      <td>0.046030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>rating</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.189837</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.844366</td>\n",
       "      <td>0.072236</td>\n",
       "      <td>0.047303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>control</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1.417118</td>\n",
       "      <td>29.7</td>\n",
       "      <td>4.373339</td>\n",
       "      <td>0.060849</td>\n",
       "      <td>0.047555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>analyze</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.116691</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.083782</td>\n",
       "      <td>0.047591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>duration</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.114141</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.420352</td>\n",
       "      <td>0.071845</td>\n",
       "      <td>0.048632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>precede</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.011443</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.209223</td>\n",
       "      <td>0.058597</td>\n",
       "      <td>0.053958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>performance</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.130628</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.070814</td>\n",
       "      <td>0.037286</td>\n",
       "      <td>0.054601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>versus</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.126394</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.425882</td>\n",
       "      <td>0.073023</td>\n",
       "      <td>0.055750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>open</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.011443</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.121186</td>\n",
       "      <td>0.055286</td>\n",
       "      <td>0.056736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.139251</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.375406</td>\n",
       "      <td>0.114122</td>\n",
       "      <td>0.057528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.114626</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.751231</td>\n",
       "      <td>0.052091</td>\n",
       "      <td>0.059517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>affect</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.461529</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.390140</td>\n",
       "      <td>0.084821</td>\n",
       "      <td>0.060421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>covariate</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.012252</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.284670</td>\n",
       "      <td>0.051583</td>\n",
       "      <td>0.060947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>near</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.109703</td>\n",
       "      <td>0.076638</td>\n",
       "      <td>0.061198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>anatomy</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.141563</td>\n",
       "      <td>0.072290</td>\n",
       "      <td>0.062744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>presentation</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.331461</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.587697</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.063313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>enter</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.034665</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.163342</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.066485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>package</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.016369</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.082492</td>\n",
       "      <td>0.054371</td>\n",
       "      <td>0.067536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>twice</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.112198</td>\n",
       "      <td>0.076638</td>\n",
       "      <td>0.068976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>spatially</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.059899</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.226420</td>\n",
       "      <td>0.084232</td>\n",
       "      <td>0.070537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>classic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.088663</td>\n",
       "      <td>0.096157</td>\n",
       "      <td>0.071187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>literature</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.014170</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.196038</td>\n",
       "      <td>0.078171</td>\n",
       "      <td>0.072912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>perception</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.055311</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.264797</td>\n",
       "      <td>0.082040</td>\n",
       "      <td>0.073988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>separate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.132116</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.406904</td>\n",
       "      <td>0.098029</td>\n",
       "      <td>0.074347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>benefit</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.016369</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.076126</td>\n",
       "      <td>0.054371</td>\n",
       "      <td>0.075126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>increase</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.032270</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2.217539</td>\n",
       "      <td>0.130471</td>\n",
       "      <td>0.075497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>black</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.038604</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.127299</td>\n",
       "      <td>0.131816</td>\n",
       "      <td>0.076143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>fact</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.107728</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.300749</td>\n",
       "      <td>0.067238</td>\n",
       "      <td>0.076698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>highlight</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.028893</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.125030</td>\n",
       "      <td>0.076878</td>\n",
       "      <td>0.080101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>continuous</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.011443</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.081144</td>\n",
       "      <td>0.119840</td>\n",
       "      <td>0.080712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>lead</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.156123</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.447440</td>\n",
       "      <td>0.089051</td>\n",
       "      <td>0.081567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Token  JP-EN Mean Count  JP-EN Mean Freq  EN-EN Mean Count  \\\n",
       "1284       explore               0.2         0.028621               1.4   \n",
       "804    alternative               0.1         0.014438               0.8   \n",
       "723         active               0.5         0.079231               2.8   \n",
       "984         remove               0.5         0.073286               1.3   \n",
       "1272     detection               0.1         0.011443               0.7   \n",
       "1208          fill               0.1         0.011443               0.6   \n",
       "1216    projection               0.1         0.011443               0.6   \n",
       "1737          trio               0.1         0.012690               0.5   \n",
       "245           vary               0.5         0.065803               1.8   \n",
       "608      establish               0.3         0.037324               1.4   \n",
       "449         rating               1.2         0.189837               6.1   \n",
       "177        control               9.9         1.417118              29.7   \n",
       "865        analyze               0.8         0.116691               1.9   \n",
       "495       duration               0.8         0.114141               2.9   \n",
       "1177       precede               0.1         0.011443               1.4   \n",
       "907    performance               0.8         0.130628              11.0   \n",
       "570         versus               0.9         0.126394               3.3   \n",
       "1209          open               0.1         0.011443               0.8   \n",
       "377       relevant               1.1         0.139251               2.3   \n",
       "482       baseline               0.7         0.114626               4.8   \n",
       "234         affect               3.0         0.461529               9.0   \n",
       "1513     covariate               0.1         0.012252               1.7   \n",
       "1776          near               0.1         0.012690               0.8   \n",
       "1748       anatomy               0.1         0.012690               0.9   \n",
       "478   presentation               2.2         0.331461               3.8   \n",
       "555          enter               0.2         0.034665               1.1   \n",
       "1388       package               0.1         0.016369               0.5   \n",
       "1793         twice               0.1         0.014100               0.8   \n",
       "1485     spatially               0.4         0.059899               1.6   \n",
       "1770       classic               0.1         0.012690               0.6   \n",
       "1624    literature               0.1         0.014170               1.1   \n",
       "762     perception               0.4         0.055311               1.8   \n",
       "957       separate               1.0         0.132116               2.5   \n",
       "1422       benefit               0.1         0.016369               0.5   \n",
       "154       increase               6.9         1.032270              13.8   \n",
       "1516         black               0.3         0.038604               0.9   \n",
       "1396          fact               0.7         0.107728               2.0   \n",
       "264      highlight               0.2         0.028893               0.8   \n",
       "1239    continuous               0.1         0.011443               0.5   \n",
       "796           lead               1.1         0.156123               2.8   \n",
       "\n",
       "      EN-EN Mean Freq  P (count)  P (freq)  \n",
       "1284         0.232113   0.007005  0.011141  \n",
       "804          0.131495   0.017920  0.025221  \n",
       "723          0.425237   0.039044  0.028967  \n",
       "984          0.205185   0.046477  0.031685  \n",
       "1272         0.124554   0.045270  0.039317  \n",
       "1208         0.088988   0.054115  0.042676  \n",
       "1216         0.098515   0.054115  0.043092  \n",
       "1737         0.077946   0.054371  0.043799  \n",
       "245          0.265272   0.061052  0.045281  \n",
       "608          0.187568   0.066324  0.046030  \n",
       "449          0.844366   0.072236  0.047303  \n",
       "177          4.373339   0.060849  0.047555  \n",
       "865          0.282895   0.083782  0.047591  \n",
       "495          0.420352   0.071845  0.048632  \n",
       "1177         0.209223   0.058597  0.053958  \n",
       "907          2.070814   0.037286  0.054601  \n",
       "570          0.425882   0.073023  0.055750  \n",
       "1209         0.121186   0.055286  0.056736  \n",
       "377          0.375406   0.114122  0.057528  \n",
       "482          0.751231   0.052091  0.059517  \n",
       "234          1.390140   0.084821  0.060421  \n",
       "1513         0.284670   0.051583  0.060947  \n",
       "1776         0.109703   0.076638  0.061198  \n",
       "1748         0.141563   0.072290  0.062744  \n",
       "478          0.587697   0.109351  0.063313  \n",
       "555          0.163342   0.049900  0.066485  \n",
       "1388         0.082492   0.054371  0.067536  \n",
       "1793         0.112198   0.076638  0.068976  \n",
       "1485         0.226420   0.084232  0.070537  \n",
       "1770         0.088663   0.096157  0.071187  \n",
       "1624         0.196038   0.078171  0.072912  \n",
       "762          0.264797   0.082040  0.073988  \n",
       "957          0.406904   0.098029  0.074347  \n",
       "1422         0.076126   0.054371  0.075126  \n",
       "154          2.217539   0.130471  0.075497  \n",
       "1516         0.127299   0.131816  0.076143  \n",
       "1396         0.300749   0.067238  0.076698  \n",
       "264          0.125030   0.076878  0.080101  \n",
       "1239         0.081144   0.119840  0.080712  \n",
       "796          0.447440   0.089051  0.081567  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lemmas less common in JP-EN than EN-EN\n",
    "toks_en = toks_df[toks_df[\"JP-EN Mean Freq\"] < toks_df[\"EN-EN Mean Freq\"]]\n",
    "toks_en.sort_values(\"P (freq)\").head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb67db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandbox\n",
    "\n",
    "# new_words1, new_words2 = [], []\n",
    "# words = ['gyrus', 'parietal-temporal-occipital', 'anxiety/depression']\n",
    "# for word in words:\n",
    "#     new_words += word.split('-')\n",
    "\n",
    "# new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e1194",
   "metadata": {},
   "source": [
    "### For counting 1-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6494c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexeme_counter(doc, string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function for getting raw lemma count in a document.\n",
    "    \"\"\"\n",
    "        \n",
    "    tokens = preprocess_tok(doc)\n",
    "    counter = 0\n",
    "    for tok in tokens:\n",
    "        if str(tok) == str(string):\n",
    "            counter += 1\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276eba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = initialize_results_df()\n",
    "\n",
    "for lexeme in [\"think\", \"consider\", \"report\"]:\n",
    "    \n",
    "    new_name_n = \"n_\" + str(lexeme)\n",
    "    new_name_adj = str(new_name_n) + \"_adj\"\n",
    "    \n",
    "    for i in range(0,20):\n",
    "        # Pull text from df\n",
    "        text = corpus_new.loc[i, \"Text\"]\n",
    "        # Run scispaCy model\n",
    "        doc = nlp(text)\n",
    "        # Count lexeme (includes preprocessing)\n",
    "        ct = lexeme_counter(doc, lexeme)\n",
    "        # Put in results_df\n",
    "        results_df.loc[i, new_name_n] = ct\n",
    "        results_df.loc[i, new_name_adj] = (ct / results_df.loc[i, 'Word Count']) #* np.mean(results_df['Word Count'])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_means('n_think')\n",
    "compare_means('n_consider')\n",
    "compare_means('n_think_adj')\n",
    "compare_means('n_consider_adj')\n",
    "compare_means('n_report')\n",
    "compare_means('n_report_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf87b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    results_df.loc[i, 'Combined'] = results_df.loc[i, 'n_think'] + results_df.loc[i, 'n_consider']\n",
    "    results_df.loc[i, 'Combined_adj'] = results_df.loc[i, 'Combined'] / results_df.loc[i, 'Word Count']\n",
    "    \n",
    "compare_means('Combined')\n",
    "compare_means('Combined_adj')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20992527",
   "metadata": {},
   "source": [
    "### For 2,3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = initialize_results_df()\n",
    "cols = []\n",
    "\n",
    "for lexeme in [\"in this study\", \"therefore\", \"in addition\", \"bold response\"]:\n",
    "    \n",
    "    new_name_n = \"n_\" + str(lexeme.replace(\" \", \"\"))\n",
    "    new_name_adj = str(new_name_n) + \"_adj\"\n",
    "    cols.append(new_name_n)\n",
    "    cols.append(new_name_adj)\n",
    "\n",
    "    for i in range(0,20):\n",
    "        text = corpus_new.loc[i, \"Text\"].lower()\n",
    "        ct = text.count(lexeme)\n",
    "        # Put in results_df\n",
    "        results_df.loc[i, new_name_n] = ct\n",
    "        results_df.loc[i, new_name_adj] = (ct / results_df.loc[i, 'Word Count']) #* np.mean(results_df['Word Count'])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad86023",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    compare_means(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7bd13a",
   "metadata": {},
   "source": [
    "### Statistical testing\n",
    "#### 1. Type/token ratio (lexical diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_token_ratio(doc):\n",
    "    \n",
    "    token_list = preprocess_tok(doc)\n",
    "    n_type = len(unique(token_list))\n",
    "    n_token = len(token_list)\n",
    "    ttr = n_type/n_token\n",
    "    \n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4497a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_means(var):\n",
    "    \n",
    "    from scipy.stats import ttest_ind\n",
    "    \n",
    "    jp_stats = list(results_df[results_df['Group'] == 'JP-EN'].loc[:, var])\n",
    "    en_stats = list(results_df[results_df['Group'] == 'EN-EN'].loc[:, var])\n",
    "    P = ttest_ind(jp_stats, en_stats).pvalue\n",
    "    \n",
    "    print(f'Mean {var}, JP-EN:  {np.mean(jp_stats)}')\n",
    "    print(f'Mean {var}, EN-EN:  {np.mean(en_stats)}')\n",
    "    print(f'Sig. (unpaired t-test): {P}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate raw type/token ratio (lemmatized)\n",
    "\n",
    "for i in range(0,20):\n",
    "    # Pull text from df\n",
    "    text = corpus_new.loc[i, \"Text\"]\n",
    "    # Run scispaCy model\n",
    "    doc = nlp(text)\n",
    "    # Preprocess\n",
    "    ttr = type_token_ratio(doc)\n",
    "    # Put in results_df\n",
    "    results_df.loc[i, 'Type/Token'] = ttr\n",
    "\n",
    "results_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate adjusted type/token ratio (lemmatized) divided by word count\n",
    "\n",
    "for i in range(0,20):\n",
    "    # Pull text from df\n",
    "    text = corpus_new.loc[i, \"Text\"]\n",
    "    # Split by word (\" \" for simplicity)\n",
    "    word_ct = len(text.split(' '))\n",
    "    # Put in results_df\n",
    "    results_df.loc[i, 'Word Count'] = word_ct\n",
    "    \n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide by mean word count of all documents\n",
    "results_df['TTR_adj'] = (results_df['Type/Token'] / results_df['Word Count']) * np.mean(results_df['Word Count'])\n",
    "    \n",
    "results_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82bf425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mean stats between JP-EN and EN-EN corpora\n",
    "\n",
    "compare_means('Type/Token')\n",
    "compare_means('Word Count')\n",
    "compare_means('TTR_adj')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9949952",
   "metadata": {},
   "source": [
    "Comments (4/10): H1 seems to be rejected. fMRI studies authored by Japanese scientists are just as lexically sophisticated as comparable docs authored by Anglophone counterparts. Perhaps this is a good thing: i.e., any differences discovered later are a product of linguistic features, rather than scientific knowledge/ignorance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4959e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9610f4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d84c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a5e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the entities extracted by the mention detector. Note that they don't have types\n",
    "# like in SpaCy, and they are more general (e.g including verbs) - these are any spans\n",
    "# which might be an entity in UMLS, a large biomedical database.\n",
    "print(a.ents)\n",
    "\n",
    "\n",
    "#>>> (Myeloid derived suppressor cells,\n",
    "#     MDSC,\n",
    "#     immature,\n",
    "#     myeloid cells,\n",
    "#     immunosuppressive activity,\n",
    "#     accumulate,\n",
    "#     tumor-bearing mice,\n",
    "#     humans,\n",
    "#     cancer,\n",
    "#     hepatocellular carcinoma,\n",
    "#     HCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ac3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also visualise dependency parses\n",
    "# (This renders automatically inside a jupyter notebook!):\n",
    "from spacy import displacy\n",
    "# displacy.render(next(doc2.sents), style='dep', jupyter=True)\n",
    "displacy.render(a, style='dep', jupyter=True)\n",
    "\n",
    "# See below for the generated SVG.\n",
    "# Zoom your browser in a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc01426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a1887e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd250a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69ef5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa559c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d1a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582371e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15373c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c4c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be52c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2734ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba911c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7371bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8562bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71c0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
