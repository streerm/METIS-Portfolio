{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOS One search results (BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search query\n",
    "# Search terms: \"fMRI study\", \"language\", -\"china\", \"-korea\"\n",
    "# Date range: 2012-01-01 to 2021-12-31 (10 years)\n",
    "\n",
    "# Full URL is 'https://journals.plos.org/plosone/search/feed/atom?filterStartDate=2012-01-01&q=%22fmri+study%22+%22language%22+-china+-korea&filterSubjects=Functional+magnetic+resonance+imaging&filterArticleTypes=Research+Article&sortOrder=DATE_NEWEST_FIRST&filterJournals=PLoSONE&page=1&filterEndDate=2021-12-31'\n",
    "# Leave the end date open for appending later\n",
    "url_base = 'https://journals.plos.org/plosone/search/feed/atom?filterStartDate=2012-01-01&q=%22fmri+study%22+%22language%22+-china+-korea&filterSubjects=Functional+magnetic+resonance+imaging&filterArticleTypes=Research+Article&sortOrder=DATE_NEWEST_FIRST&filterJournals=PLoSONE&page=1&filterEndDate='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since appending '&page=2', '=3' etc. doesn't shorten the query results,\n",
    "# We cut off the most recent results by stepping \"filterEndDate\" by two months\n",
    "\n",
    "years = [str(x) for x in range(2021,2011,-1)]\n",
    "months = [str(x) for x in range(12,0,-2)]\n",
    "day = '28'\n",
    "\n",
    "monthdays = [month + '-' + day for month in months]\n",
    "\n",
    "enddates = []\n",
    "\n",
    "for year in years:\n",
    "    for monthday in monthdays:\n",
    "        enddate = year + '-' + monthday\n",
    "        enddates.append(enddate)\n",
    "\n",
    "# Now for the complete list of URLs\n",
    "search_url_list = [url_base + enddate for enddate in enddates]\n",
    "\n",
    "# Set up 'Subject Area' labels for later\n",
    "SA_keys = ['SA'+str(x) for x in range(1,9)]  #'SA1', 'SA2', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(res_url):\n",
    "    \n",
    "    response = requests.get(res_url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    df = pd.DataFrame([], columns = SA_keys)\n",
    "    \n",
    "    for entry in soup.find_all('entry'):        \n",
    "        Title = {'Title': entry.title.text}\n",
    "        url = str(entry.link)[12:85]\n",
    "        URL = {'URL': url}\n",
    "        \n",
    "        INFO = {}\n",
    "        INFO.update(Title)\n",
    "        INFO.update(URL)\n",
    "        \n",
    "        # Get the 8 \"Subject Areas\" given for each PLOS One article\n",
    "        # Note 'url' is for a specific article; 'res_url' for query results page of 15 articles\n",
    "\n",
    "        response = requests.get(url)\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "        SA_vals = []\n",
    "\n",
    "        for entry in soup.find_all('a', class_='taxo-term'): \n",
    "            term = entry.text\n",
    "            SA_vals.append(term)\n",
    "        \n",
    "        SAs = dict(zip(SA_keys, SA_vals))\n",
    "        INFO.update(SAs)\n",
    "        \n",
    "        df_row = pd.DataFrame([INFO])\n",
    "        df = pd.concat([df, df_row])\n",
    "           \n",
    "    return df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the scraping\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i, url in enumerate(search_url_list):\n",
    "    df_scrape = get_article_info(url)\n",
    "    df = pd.concat([df, df_scrape])\n",
    "    df = df.drop_duplicates(subset=['Title'], keep='first')\n",
    "    \n",
    "    wait = .5 + 10 * random.random()\n",
    "    time.sleep(wait)\n",
    "    print(f'{enddates[i]} - waited {wait:0.4} sec.')\n",
    "\n",
    "df_en = df[['Title', 'URL'] + SA_keys]\n",
    "df_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For JP articles\n",
    "\n",
    "df_jp = pd.read_csv('refs/jprefs.csv')\n",
    "\n",
    "jp_titles = list(df_jp['Title'])\n",
    "jp_urls = list(df_jp['URL'])\n",
    "\n",
    "df_jp = pd.DataFrame()\n",
    "\n",
    "for i, url in enumerate(jp_urls):\n",
    "    \n",
    "    INFO = {}\n",
    "\n",
    "    Title = {'Title': jp_titles[i]}\n",
    "    URL = {'URL': url}\n",
    "    \n",
    "    INFO.update(Title)\n",
    "    INFO.update(URL)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "    SA_keys = ['SA'+str(x) for x in range(1,9)]  #'SA1', 'SA2', etc.\n",
    "    SA_vals = []\n",
    "\n",
    "    for entry in soup.find_all('a', class_='taxo-term'): \n",
    "        term = entry.text\n",
    "        SA_vals.append(term)\n",
    "\n",
    "    SAs = dict(zip(SA_keys, SA_vals))\n",
    "    INFO.update(SAs)\n",
    "\n",
    "    df_row = pd.DataFrame([INFO])\n",
    "    df_jp = pd.concat([df_jp, df_row])\n",
    "\n",
    "df_jp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your work\n",
    "df_en.to_pickle('savefiles/df_en.pkl')\n",
    "df_jp.to_pickle('savefiles/df_jp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining EN articles closest to JP articles by keyword (SA) similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your work\n",
    "df_en = pd.read_pickle('savefiles/df_en.pkl')\n",
    "df_jp = pd.read_pickle('savefiles/df_jp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "df_en['ID'] = 'EN'\n",
    "df_en = df_en[['ID'] + list(df_en.columns)]\n",
    "df_en = df_en.reset_index(drop=True)\n",
    "df_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "df_jp['ID'] = 'JP'\n",
    "df_jp = df_jp[['ID'] + list(df_jp.columns)]\n",
    "df_jp = df_jp.reset_index(drop=True)\n",
    "df_jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each JP article will be searched in turn\n",
    "# Every EN article will need to be 'scored' with respect to each JP row\n",
    "keywords_jp = set(df_jp.loc[0, SA_keys].values)\n",
    "keywords_jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_df_en_by_jp_article(keywords_jp):\n",
    "\n",
    "    for index, row in df_en.iterrows():\n",
    "        score = 0\n",
    "        keywords = list(row[SA_keys])\n",
    "        for word in keywords:\n",
    "            if word in keywords_jp:\n",
    "                score += 1\n",
    "        df_en.loc[index, 'SCORE'] = score\n",
    "        \n",
    "    df_sorted = df_en.sort_values(by='SCORE', ascending=False)\n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_jp.iterrows():\n",
    "    keywords_jp = set(df_jp.loc[index, SA_keys].values)\n",
    "    df_score = score_df_en_by_jp_article(keywords_jp)\n",
    "    print(f\"{index}. {row['Title']}\")\n",
    "    print(df_score[['Title', 'URL', 'SCORE']].iloc[:5, :].values)\n",
    "    \n",
    "# Partner papers decided based on this output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
